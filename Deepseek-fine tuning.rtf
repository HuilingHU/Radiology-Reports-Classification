{\rtf1\ansi\ansicpg936\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue255;\red245\green245\blue245;\red0\green0\blue0;
\red19\green118\blue70;\red131\green0\blue165;\red144\green1\blue18;\red86\green65\blue25;\red0\green0\blue109;
\red19\green85\blue52;\red31\green99\blue128;\red144\green1\blue18;\red245\green245\blue245;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c100000;\cssrgb\c96863\c96863\c96863;\cssrgb\c0\c0\c0;
\cssrgb\c3529\c52549\c34510;\cssrgb\c59216\c13725\c70588;\cssrgb\c63922\c8235\c8235;\cssrgb\c41569\c32157\c12941;\cssrgb\c0\c6275\c50196;
\cssrgb\c6667\c40000\c26667;\cssrgb\c14510\c46275\c57647;\cssrgb\c63922\c8235\c8235;\cssrgb\c96863\c96863\c96863;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ## Deepseek fine-tuning\
\
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 !\cf0 \strokec4 pip uninstall -y transformers peft accelerate bitsandbytes trl\cb1 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install transformers==\cf5 \strokec5 4.41.2\cf0 \strokec4  peft accelerate bitsandbytes\cb1 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install peft==\cf5 \strokec5 0.8.2\cf0 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install accelerate==\cf5 \strokec5 0.27.2\cf0 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install bitsandbytes\cb1 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install trl==\cf5 \strokec5 0.7.10\cf0 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install datasets\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  os\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb3 os.environ[\cf7 \cb3 \strokec7 "PYTORCH_CUDA_ALLOC_CONF"\cf0 \cb3 \strokec4 ] = \cf7 \cb3 \strokec7 "expandable_segments:True"\cf0 \cb1 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  huggingface_hub \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  login\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb3 login()\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  transformers \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\cb1 \
\cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  torch\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb3 model_name = \cf7 \cb3 \strokec7 "deepseek-ai/deepseek-llm-7b-chat"\cf0 \cb1 \strokec4 \
\cb3 tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\cf2 \strokec2 True\cf0 \strokec4 )\cb1 \
\cb3 tokenizer.pad_token = tokenizer.eos_token\cb1 \
\cb3 tokenizer.padding_side = \cf7 \cb3 \strokec7 "right"\cf0 \cb1 \strokec4 \
\cb3 bnb_config = BitsAndBytesConfig(\cb1 \
\cb3     load_in_4bit=\cf2 \strokec2 True\cf0 \strokec4 ,\cb1 \
\cb3     bnb_4bit_quant_type=\cf7 \cb3 \strokec7 "nf4"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     bnb_4bit_use_double_quant=\cf2 \strokec2 True\cf0 \strokec4 ,\cb1 \
\cb3     bnb_4bit_compute_dtype=torch.bfloat16\cb1 \
\cb3 )\cb1 \
\cb3 model = AutoModelForCausalLM.from_pretrained(\cb1 \
\cb3     model_name,\cb1 \
\cb3     device_map=\cf7 \cb3 \strokec7 "auto"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     quantization_config=bnb_config,\cb1 \
\cb3     trust_remote_code=\cf2 \strokec2 True\cf0 \cb1 \strokec4 \
\cb3 )\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  datasets \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  load_dataset, DatasetDict\cb1 \
\
\cb3 dataset = DatasetDict(\{\cb1 \
\cb3     \cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 : load_dataset(\cf7 \cb3 \strokec7 "json"\cf0 \cb3 \strokec4 , data_files=\cf7 \cb3 \strokec7 "train1.jsonl"\cf0 \cb3 \strokec4 )[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3     \cf7 \cb3 \strokec7 "validation"\cf0 \cb3 \strokec4 : load_dataset(\cf7 \cb3 \strokec7 "json"\cf0 \cb3 \strokec4 , data_files=\cf7 \cb3 \strokec7 "val1.jsonl"\cf0 \cb3 \strokec4 )[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3     \cf7 \cb3 \strokec7 "test"\cf0 \cb3 \strokec4 : load_dataset(\cf7 \cb3 \strokec7 "json"\cf0 \cb3 \strokec4 , data_files=\cf7 \cb3 \strokec7 "test1_infer.jsonl"\cf0 \cb3 \strokec4 )[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3 \})\cb1 \
\
\cf2 \cb3 \strokec2 def\cf0 \strokec4  \cf8 \cb3 \strokec8 formatting_prompts_train\cf0 \cb3 \strokec4 (\cf9 \cb3 \strokec9 example\cf0 \cb3 \strokec4 ):\cb1 \
\cb3     messages = example[\cf7 \cb3 \strokec7 "messages"\cf0 \cb3 \strokec4 ]\cb1 \
\cb3     user_msg = \cf8 \cb3 \strokec8 next\cf0 \cb3 \strokec4 ((m[\cf7 \cb3 \strokec7 "content"\cf0 \cb3 \strokec4 ] \cf6 \cb3 \strokec6 for\cf0 \cb3 \strokec4  m \cf2 \strokec2 in\cf0 \strokec4  messages \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  m[\cf7 \cb3 \strokec7 "role"\cf0 \cb3 \strokec4 ] == \cf7 \cb3 \strokec7 "user"\cf0 \cb3 \strokec4 ), \cf2 \strokec2 None\cf0 \strokec4 )\cb1 \
\cb3     assistant_msg = \cf8 \cb3 \strokec8 next\cf0 \cb3 \strokec4 ((m[\cf7 \cb3 \strokec7 "content"\cf0 \cb3 \strokec4 ] \cf6 \cb3 \strokec6 for\cf0 \cb3 \strokec4  m \cf2 \strokec2 in\cf0 \strokec4  messages \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  m[\cf7 \cb3 \strokec7 "role"\cf0 \cb3 \strokec4 ] == \cf7 \cb3 \strokec7 "assistant"\cf0 \cb3 \strokec4 ), \cf2 \strokec2 None\cf0 \strokec4 )\cb1 \
\
\cb3     \cf6 \cb3 \strokec6 return\cf0 \cb3 \strokec4  \{\cb1 \
\cb3         \cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 : example[\cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3         \cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 : user_msg,\cb1 \
\cb3         \cf7 \cb3 \strokec7 "response"\cf0 \cb3 \strokec4 : assistant_msg\cb1 \
\cb3     \}\cb1 \
\
\cf2 \cb3 \strokec2 def\cf0 \strokec4  \cf8 \cb3 \strokec8 formatting_prompts_test\cf0 \cb3 \strokec4 (\cf9 \cb3 \strokec9 example\cf0 \cb3 \strokec4 ):\cb1 \
\cb3     messages = example[\cf7 \cb3 \strokec7 "messages"\cf0 \cb3 \strokec4 ]\cb1 \
\cb3     user_msg = \cf8 \cb3 \strokec8 next\cf0 \cb3 \strokec4 ((m[\cf7 \cb3 \strokec7 "content"\cf0 \cb3 \strokec4 ] \cf6 \cb3 \strokec6 for\cf0 \cb3 \strokec4  m \cf2 \strokec2 in\cf0 \strokec4  messages \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  m[\cf7 \cb3 \strokec7 "role"\cf0 \cb3 \strokec4 ] == \cf7 \cb3 \strokec7 "user"\cf0 \cb3 \strokec4 ), \cf2 \strokec2 None\cf0 \strokec4 )\cb1 \
\cb3     \cf6 \cb3 \strokec6 return\cf0 \cb3 \strokec4  \{\cb1 \
\cb3         \cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 : example[\cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3         \cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 : user_msg\cb1 \
\cb3     \}\cb1 \
\
\cb3 dataset[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ] = dataset[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ].\cf8 \cb3 \strokec8 map\cf0 \cb3 \strokec4 (formatting_prompts_train, remove_columns=dataset[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ].column_names)\cb1 \
\cb3 dataset[\cf7 \cb3 \strokec7 "validation"\cf0 \cb3 \strokec4 ] = dataset[\cf7 \cb3 \strokec7 "validation"\cf0 \cb3 \strokec4 ].\cf8 \cb3 \strokec8 map\cf0 \cb3 \strokec4 (formatting_prompts_train, remove_columns=dataset[\cf7 \cb3 \strokec7 "validation"\cf0 \cb3 \strokec4 ].column_names)\cb1 \
\cb3 dataset[\cf7 \cb3 \strokec7 "test"\cf0 \cb3 \strokec4 ] = dataset[\cf7 \cb3 \strokec7 "test"\cf0 \cb3 \strokec4 ].\cf8 \cb3 \strokec8 map\cf0 \cb3 \strokec4 (formatting_prompts_test, remove_columns=dataset[\cf7 \cb3 \strokec7 "test"\cf0 \cb3 \strokec4 ].column_names)\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 !\cf0 \strokec4 pip install trl==\cf5 \strokec5 0.8.1\cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  peft \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  LoraConfig, get_peft_model, prepare_model_for_kbit_training\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  transformers \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  TrainingArguments\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  trl \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  SFTTrainer\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  transformers \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  DataCollatorForLanguageModeling\cb1 \
\cb3 lora_config = LoraConfig(\cb1 \
\cb3     r=\cf10 \cb3 \strokec10 16\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     lora_alpha=\cf10 \cb3 \strokec10 32\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     target_modules=[\cf7 \cb3 \strokec7 "q_proj"\cf0 \cb3 \strokec4 , \cf7 \cb3 \strokec7 "k_proj"\cf0 \cb3 \strokec4 , \cf7 \cb3 \strokec7 "v_proj"\cf0 \cb3 \strokec4 , \cf7 \cb3 \strokec7 "o_proj"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3     lora_dropout=\cf10 \cb3 \strokec10 0.05\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     bias=\cf7 \cb3 \strokec7 "none"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     task_type=\cf7 \cb3 \strokec7 "CAUSAL_LM"\cf0 \cb1 \strokec4 \
\cb3 )\cb1 \
\cb3 model = prepare_model_for_kbit_training(model)\cb1 \
\cb3 model = get_peft_model(model, lora_config)\cb1 \
\cb3 training_args = TrainingArguments(\cb1 \
\cb3     output_dir=\cf7 \cb3 \strokec7 "./deepseek_lora_finetune"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     per_device_train_batch_size=\cf10 \cb3 \strokec10 1\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     per_device_eval_batch_size=\cf10 \cb3 \strokec10 1\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     gradient_accumulation_steps=\cf10 \cb3 \strokec10 8\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     evaluation_strategy=\cf7 \cb3 \strokec7 "steps"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     eval_steps=\cf10 \cb3 \strokec10 100\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     save_steps=\cf10 \cb3 \strokec10 200\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     save_total_limit=\cf10 \cb3 \strokec10 1\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     num_train_epochs=\cf10 \cb3 \strokec10 3\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     learning_rate=\cf10 \cb3 \strokec10 2e-4\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     warmup_ratio=\cf10 \cb3 \strokec10 0.1\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     logging_steps=\cf10 \cb3 \strokec10 10\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     bf16=\cf2 \strokec2 True\cf0 \strokec4 ,\cb1 \
\cb3     fp16=\cf2 \strokec2 False\cf0 \strokec4 ,\cb1 \
\cb3     report_to=\cf7 \cb3 \strokec7 "none"\cf0 \cb1 \strokec4 \
\cb3 )\cb1 \
\cb3 trainer = SFTTrainer(\cb1 \
\cb3     model=model,\cb1 \
\cb3     args=training_args,\cb1 \
\cb3     train_dataset=dataset[\cf7 \cb3 \strokec7 "train"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3     eval_dataset=dataset[\cf7 \cb3 \strokec7 "validation"\cf0 \cb3 \strokec4 ],\cb1 \
\cb3     tokenizer=tokenizer,\cb1 \
\cb3     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=\cf2 \strokec2 False\cf0 \strokec4 ),\cb1 \
\cb3     dataset_text_field=\cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     max_seq_length=\cf10 \cb3 \strokec10 1024\cf0 \cb1 \strokec4 \
\cb3 )\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb3 train_losses = []\cb1 \
\cb3 val_losses = []\cb1 \
\cb3 steps = []\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  transformers \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  TrainerCallback\cb1 \
\cf2 \cb3 \strokec2 class\cf0 \strokec4  \cf11 \cb3 \strokec11 LossRecorderCallback\cf0 \cb3 \strokec4 (\cf11 \cb3 \strokec11 TrainerCallback\cf0 \cb3 \strokec4 ):\cb1 \
\cb3     \cf2 \strokec2 def\cf0 \strokec4  \cf8 \cb3 \strokec8 on_log\cf0 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf0 \cb3 \strokec4 , \cf9 \cb3 \strokec9 args\cf0 \cb3 \strokec4 , \cf9 \cb3 \strokec9 state\cf0 \cb3 \strokec4 , \cf9 \cb3 \strokec9 control\cf0 \cb3 \strokec4 , \cf9 \cb3 \strokec9 logs\cf0 \cb3 \strokec4 =\cf2 \strokec2 None\cf0 \strokec4 , **\cf9 \cb3 \strokec9 kwargs\cf0 \cb3 \strokec4 ):\cb1 \
\cb3         \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  logs \cf2 \strokec2 is\cf0 \strokec4  \cf2 \strokec2 not\cf0 \strokec4  \cf2 \strokec2 None\cf0 \strokec4 :\cb1 \
\cb3             \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  \cf7 \cb3 \strokec7 "loss"\cf0 \cb3 \strokec4  \cf2 \strokec2 in\cf0 \strokec4  logs:\cb1 \
\cb3                 train_losses.append(logs[\cf7 \cb3 \strokec7 "loss"\cf0 \cb3 \strokec4 ])\cb1 \
\cb3                 steps.append(state.global_step)\cb1 \
\cb3             \cf6 \cb3 \strokec6 if\cf0 \cb3 \strokec4  \cf7 \cb3 \strokec7 "eval_loss"\cf0 \cb3 \strokec4  \cf2 \strokec2 in\cf0 \strokec4  logs:\cb1 \
\cb3                 val_losses.append(logs[\cf7 \cb3 \strokec7 "eval_loss"\cf0 \cb3 \strokec4 ])\cb1 \
\cb3 trainer.add_callback(LossRecorderCallback)\cb1 \
\cb3 trainer.train()\cb1 \
\cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  matplotlib.pyplot \cf6 \cb3 \strokec6 as\cf0 \cb3 \strokec4  plt\cb1 \
\cb3 plt.figure(figsize=(\cf10 \cb3 \strokec10 10\cf0 \cb3 \strokec4 , \cf10 \cb3 \strokec10 6\cf0 \cb3 \strokec4 ))\cb1 \
\cb3 plt.plot(steps, train_losses, label=\cf7 \cb3 \strokec7 "Training Loss"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 plt.plot(steps[:\cf8 \cb3 \strokec8 len\cf0 \cb3 \strokec4 (val_losses)], val_losses, label=\cf7 \cb3 \strokec7 "Validation Loss"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 plt.xlabel(\cf7 \cb3 \strokec7 "Step"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 plt.ylabel(\cf7 \cb3 \strokec7 "Loss"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 plt.title(\cf7 \cb3 \strokec7 "Training and Validation Loss during DeepSeek LoRA Fine-tuning"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 plt.legend()\cb1 \
\cb3 plt.grid()\cb1 \
\cb3 plt.tight_layout()\cb1 \
\cb3 plt.show()\cb1 \
\
\cb3 output_dir = \cf7 \cb3 \strokec7 "./deepseek_lora_finetune_adapter"\cf0 \cb1 \strokec4 \
\cb3 model.save_pretrained(output_dir)\cb1 \
\cb3 tokenizer.save_pretrained(output_dir)\cb1 \
\cf8 \cb3 \strokec8 print\cf0 \cb3 \strokec4 (\cf2 \strokec2 f\cf7 \cb3 \strokec7 "LoRA adapter have saved to\uc0\u65306 \cf0 \cb3 \strokec4 \{output_dir\}\cf7 \cb3 \strokec7 "\cf0 \cb3 \strokec4 )\cb1 \
\
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  peft \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  PeftModel\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  transformers \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  AutoModelForCausalLM, AutoTokenizer\cb1 \
\cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  torch\cb1 \
\cb3 model_name = \cf7 \cb3 \strokec7 "deepseek-ai/deepseek-llm-7b-chat"\cf0 \cb1 \strokec4 \
\cb3 tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\cf2 \strokec2 True\cf0 \strokec4 )\cb1 \
\cb3 tokenizer.pad_token = tokenizer.eos_token\cb1 \
\cb3 tokenizer.padding_side = \cf7 \cb3 \strokec7 "right"\cf0 \cb1 \strokec4 \
\cb3 base_model = AutoModelForCausalLM.from_pretrained(\cb1 \
\cb3     model_name,\cb1 \
\cb3     device_map=\cf7 \cb3 \strokec7 "auto"\cf0 \cb3 \strokec4 ,\cb1 \
\cb3     torch_dtype=torch.bfloat16,\cb1 \
\cb3     trust_remote_code=\cf2 \strokec2 True\cf0 \strokec4 ,\cb1 \
\cb3 )\cb1 \
\cb3 lora_model = PeftModel.from_pretrained(base_model, \cf7 \cb3 \strokec7 "./deepseek_lora_finetune_adapter"\cf0 \cb3 \strokec4 )\cb1 \
\cb3 lora_model.\cf8 \cb3 \strokec8 eval\cf0 \cb3 \strokec4 ()\cb1 \
\
\cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  json\cb1 \
\cf6 \cb3 \strokec6 from\cf0 \cb3 \strokec4  tqdm \cf6 \cb3 \strokec6 import\cf0 \cb3 \strokec4  tqdm\cb1 \
\cb3 output_path = \cf7 \cb3 \strokec7 "deepseek_test_outputs.jsonl"\cf0 \cb1 \strokec4 \
\cb3 error_path = \cf7 \cb3 \strokec7 "deepseek_test_errors.jsonl"\cf0 \cb1 \strokec4 \
\cf6 \cb3 \strokec6 with\cf0 \cb3 \strokec4  \cf8 \cb3 \strokec8 open\cf0 \cb3 \strokec4 (output_path, \cf7 \cb3 \strokec7 "w"\cf0 \cb3 \strokec4 , encoding=\cf7 \cb3 \strokec7 "utf-8"\cf0 \cb3 \strokec4 ) \cf6 \cb3 \strokec6 as\cf0 \cb3 \strokec4  fout, \cf8 \cb3 \strokec8 open\cf0 \cb3 \strokec4 (error_path, \cf7 \cb3 \strokec7 "w"\cf0 \cb3 \strokec4 , encoding=\cf7 \cb3 \strokec7 "utf-8"\cf0 \cb3 \strokec4 ) \cf6 \cb3 \strokec6 as\cf0 \cb3 \strokec4  ferr:\cb1 \
\cb3     \cf6 \cb3 \strokec6 for\cf0 \cb3 \strokec4  example \cf2 \strokec2 in\cf0 \strokec4  tqdm(test_dataset):\cb1 \
\cb3         \cf6 \cb3 \strokec6 try\cf0 \cb3 \strokec4 :\cb1 \
\cb3             prompt = example[\cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 ]\cb1 \
\cb3             note_id = example[\cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 ]\cb1 \
\cb3             input_text = (\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "<|User|> \cf12 \cb13 \outl0\strokewidth0 You are a medical radiology report analysis expert who is good at reading reports and extracting structured content. Please output the following structure in JSON format\cf7 \cb3 \outl0\strokewidth0 \strokec7 \\n"\cf0 \cb1 \strokec4 \
\cb3                 \cf7 \cb3 \strokec7 \'93\{\\\'94Examination type\\\'94: \\"\\", \\\'94Examination aera\\\'94: [\\"\\"], \\\'94Interventional procedure\\\'94: [\\"\\"]\}\uc0\u12290 \\n"\cf0 \cb1 \strokec4 \
\cb3                 \cf7 \cb3 \strokec7 "\uc0\u35831 \u19981 \u35201 \u37325 \u22797 \u25253 \u21578 \u21407 \u25991 \u65292 \u19981 \u35201 \u36755 \u20986 \u35299 \u37322 \u35828 \u26126 \u12290 \\n\\n"\cf0 \cb1 \strokec4 \
\cb3                 \cf2 \strokec2 f\cf7 \cb3 \strokec7 \'94Radiology Report\uc0\u65306 \cf0 \cb3 \strokec4 \{prompt.strip()\}\cf7 \cb3 \strokec7 \\n\\n<\cf0 \cb3 \strokec4 |\cf7 \cb3 \strokec7 Assistant\cf0 \cb3 \strokec4 |\cf7 \cb3 \strokec7 >"\cf0 \cb1 \strokec4 \
\cb3             )\cb1 \
\cb3             inputs = tokenizer(input_text, return_tensors=\cf7 \cb3 \strokec7 "pt"\cf0 \cb3 \strokec4 ).to(lora_model.device)\cb1 \
\cb3             outputs = lora_model.generate(\cb1 \
\cb3                 **inputs,\cb1 \
\cb3                 max_new_tokens=\cf10 \cb3 \strokec10 256\cf0 \cb3 \strokec4 ,\cb1 \
\cb3                 do_sample=\cf2 \strokec2 False\cf0 \strokec4 ,\cb1 \
\cb3                 temperature=\cf10 \cb3 \strokec10 0.0\cf0 \cb3 \strokec4 ,\cb1 \
\cb3                 eos_token_id=tokenizer.eos_token_id\cb1 \
\cb3             )\cb1 \
\cb3             reply = tokenizer.decode(outputs[\cf10 \cb3 \strokec10 0\cf0 \cb3 \strokec4 ], skip_special_tokens=\cf2 \strokec2 True\cf0 \strokec4 ).strip()\cb1 \
\cb3             fout.write(json.dumps(\{\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 : note_id,\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 : prompt,\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "reply"\cf0 \cb3 \strokec4 : reply\cb1 \
\cb3             \}, ensure_ascii=\cf2 \strokec2 False\cf0 \strokec4 ) + \cf7 \cb3 \strokec7 "\\n"\cf0 \cb3 \strokec4 )\cb1 \
\
\cb3         \cf6 \cb3 \strokec6 except\cf0 \cb3 \strokec4  Exception \cf6 \cb3 \strokec6 as\cf0 \cb3 \strokec4  e:\cb1 \
\cb3             ferr.write(json.dumps(\{\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 : example.get(\cf7 \cb3 \strokec7 "note_id"\cf0 \cb3 \strokec4 , \cf7 \cb3 \strokec7 "unknown"\cf0 \cb3 \strokec4 ),\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "prompt"\cf0 \cb3 \strokec4 : prompt,\cb1 \
\cb3                 \cf7 \cb3 \strokec7 "error"\cf0 \cb3 \strokec4 : \cf11 \cb3 \strokec11 str\cf0 \cb3 \strokec4 (e)\cb1 \
\cb3             \}, ensure_ascii=\cf2 \strokec2 False\cf0 \strokec4 ) + \cf7 \cb3 \strokec7 "\\n"\cf0 \cb3 \strokec4 )\cb1 \
\cf8 \cb3 \strokec8 print\cf0 \cb3 \strokec4 (\cf7 \cb3 \strokec7 "Output saved to\uc0\u65306 "\cf0 \cb3 \strokec4 , output_path)\cb1 \
\
}